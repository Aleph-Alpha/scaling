# Aleph Alpha Transformer

## Adding/Updating dependencies

Dependencies are stored in `requirements/*.in`.
There must always be a `_all.in` file importing every other one to ensure global package constraints resolving.

Once the `.in` files are edited, the `.txt` files can be regenerated by issuing:

```bash
# If you do not yet have pip-compile-multi
pip install pip-compile-multi

# Set your GITLAB_TOKEN as an environment variable
export GITLAB_TOKEN=...

# From the repository root
pip-compile-multi --autoresolve --extra-index-url https://__token__:${GITLAB_TOKEN}@gitlab.aleph-alpha.de/api/v4/projects/497/packages/pypi/simple
# ~ Takes a few minutes to complete ~

# You may also run from root to automatically update docker requirements.
bash update_requirements.sh $GITLAB_TOKEN

Known problems: If you see `Please add constraints for the package version listed above`, look ABOVE the ENTIRE stacktrace and you will see a message like `Package typing-extensions was resolved to different versions in different environments: 4.8.0 and 4.7.1`, this means that you need to select one of the 2 versions in all the `.in` files because they're in conflict.
```

### Acquire Gitlab Access Token

Generate a personal access token from [here](https://gitlab.aleph-alpha.de/-/profile/personal_access_tokens).

# Start a shell for debugging

Assumption: have a clone of the aleph alpha scaling and transformer on scratch_2 with potential dirty changes or checked out on a branch.

Change: determined_configs/example-shell-scaling.yml to reflect your mount (search for Hannah). Note that the resource pool 7gpu_pool is selected. Remove the line if no gpu is available in this pool or you need another. Potentially change the number of slots (gpus).

Run:
det shell start --config-file determined_configs/example-shell.yml --context . -d

# Finetuning

Make sure to set the following parameters

```yaml
{
    "training": {
        # enable parameter selection by regex
        "finetune": true,

        # regular expressions selecting all parameters for training to which it applies
        "finetunable_parameters": ["nice_new_bias"],

        # regular expression to not select parameters for which the above selection applies
        "parameters_exclude": ["nice_new_bias_from_previous_training"],
    },
    "trainer": {

        # optimizer states will not be valid
        "load_optimizer_states": false,

        # context will not be valid
        # not loading the context will reset steps to 0
        # this is also needed to ensure the right learning rate scheduler
        "load_context": false,
        "assert_checkpoint_loaded": true,

        # if initializing new parameters that are not in the checkpoint already
        # you may add them to allowed_missing_keys_in_checkpoint
        # allowed missing keys will be matched by regex
        "allowed_missing_keys_in_checkpoint": [],

        # if the checkpoint contains more parameters than are to be used
        # you may remove them from the config and not load
        # allowed unexpected keys will be matched by regex
        # This example will exclude all finetuning parameters from transformer base
        "allowed_unexpected_keys_in_checkpoint": [
            "image_encoder",
            "symmetric", # applies to both asymmetric and symmetric...
            "softprompt_simplification",
        ],
    },

    # add your configuration
    # all these are examples
    "transformer_architecture": {
        "image_encoder": false, # just a bool for default clip
        "bias_name": "nice_new_bias", # set your bias name if it is to be used constantly in the run, you will get an error of not receiving grads otherwise
        "bias_names": [
            "nice_new_bias","symmetric", "asymmetric", "symmetric_128", "asymmetric_128", "image_encoder"
        ],
        "softprompt_name": null, # set your softprompt name if it is to be used constantly in the run, you will get an error of not receiving grads otherwise
        "softprompts": [
            {"name": "simplification", "n_tokens": 16},
        ],
        "adapter_name": null, # set your adapter name if it is to be used constantly in the run, you will get an error of not receiving grads otherwise
        "adapters": [
            {"name": "image_encoder", "attention_downsampling_factor": 0.25, "mlp_downsampling_factor": 0.25}
        ],

        # currently finetuning on embedding heads is not implemented
        "embedding_heads": [
            {"name": "symmetric_128", "proj_layers": [128]},
            {"name": "asymmetric_128", "proj_layers": [128]},
            {"name": "symmetric", "proj_layers": []},
            {"name": "asymmetric", "proj_layers": []}
        ],
    },
},
```

### Long Sequences Adaption

In order to do long sequences fine-tuning, we have to drop all the samples which are short and not fully filling up the model Context. We can do that by setting `only_full_sequences` to `True`.

```yaml
{
    "data": {
        "only_full_sequences": True,
    }
}
```

Ok, but maybe we are afraid that only training on long sequence data might shift our model in a direction we don't want it to go. Luckily, there is an option to also still have every N-th sequence to not fully fill up the context of non-max-sequence length data.

```yaml
{
    "data": {
        "only_full_sequences": True,
        "allow_incomplete_sequences_every_n": 4,
    }
}
```

If we set `allow_incomplete_sequences_every_n` to `4`, that means for every 3 full context sequences we will have one (the fourth one) that will be pieced together by shorter sequences.

###### Note
Setting `only_full_sequences` or changing `allow_incomplete_sequences_every_n` will result in a re-computation of the dataset and blended dataset indexes if they are not already existing. Also, make sure to not forget to change the model sequence lengths to the larger target size. ðŸ˜‰


### Checkpoints

The latest version of the `aleph-alpha-scaling` library includes this trainer config enabled by default: `delete_past_optimizer_states` which deletes the optimizer states for all the checkpoints except the last one during training. If you wish to disable this, you need to do so manually by setting `delete_past_optimizer_states` to `False` in the `trainer` section of the training config.

### Metrics logging

Scaling supports metrics logging to weights and biases, determined (if started via the `src/transformer/train_determined.py` script), and tensorboard. For tensorboard in specific, this has caused multiple issues regarding writing to individual files for each step making it hard to back up data in pfss. Hence, we have now an additional environment variable `SCALING_ENABLE_TENSORBOARD`, and we only write tensorboard if this environment variable is set, **and** if the tensorboard flag is set to true on the logger.
