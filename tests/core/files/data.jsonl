{"text": "Ein Transformer ist eine Methode, mit der ein Computer eine Folge von Zeichen in eine andere Folge von Zeichen übersetzen kann. Dies kann z. B. benutzt werden, um Text von einer Sprache in eine andere zu übersetzen. Dazu wird ein Transformer mittels maschinellem Lernen auf einer (großen) Menge von Beispiel-Daten trainiert, bevor das trainierte Modell dann zur Übersetzung verwendet werden kann. Transformer gehören zu den Deep-Learning-Architekturen. Transformer wurden 2017 im Rahmen der Neural-Information-Processing-Systems-Konferenz veröffentlicht. Weitere Beispielanwendungen von Transformern sind die Textgenerierung oder die Zusammenfassung längerer Texte. Transformer weisen hierbei eine bessere Effizienz gegenüber Long-short-term-memory-Architekturen (LSTM) auf und sind die Grundarchitektur vieler vortrainierter Machine-Learning-Modelle wie Bidirectional Encoder Representations from Transformers (BERT)[1] und Generative Pretrained Transformer (GPT). Vor der Einführung des Transformers wurden in der Verarbeitung natürlicher Sprache (NLP) rekurrente Modelle wie Lstm, GRU und Seq2Seq eingesetzt, welche eine Eingangssequenz sequenziell abarbeiten. Diese Methoden wurden später durch einen Aufmerksamkeitsmechanismus (engl. attention) erweitert. Transformer bauen auf dem Aufmerksamkeitsmechanismus auf und verzichten auf die rekurrente Struktur. Sie erzielen bei geringerem Rechenaufwand ähnliche oder bessere Ergebnisse bei der Transformation von Sequenzen als rekurrente Modelle."}
{"text": "in Transformer besteht im Wesentlichen aus in Serie geschalteten Kodierern (Encoder) und in Serie geschalteten Dekodierern (Decoder).[2][3][4] Die Eingabesequenz wird durch eine sogenannte Embedding-Schicht in eine Vektorrepräsentation überführt. Die Gewichte der Embedding-Schicht werden während des Trainings angepasst. Im Falle des Transformers kommt zusätzlich eine Positionskodierung zum Einsatz, wodurch die sequentielle Abfolge der Wörter berücksichtigt werden kann. Ein Wort erhält somit zu Beginn eines Satzes eine andere Repräsentation als am Ende.[3] Die Eingabesequenz wird in der Vektorrepräsentation einer Serie von Kodierern übergeben und in eine interne Repräsentation überführt. Diese interne Repräsentation bildet die Bedeutung der Eingabesequenz abstrakt ab[4] und wird durch die Dekodierer in eine Ausgabesequenz übersetzt.[4] Die Eingabesequenz wird hierbei in Batches verarbeitet, wobei die Länge der Kodierer-Dekodierer-Pipeline die maximale Länge der Eingabesequenz beschränkt.[3] Je nach Größe des Netzwerks können beispielsweise einzelne Sätze oder auch ganze Absätze verarbeitet werden. Bei Eingabesequenzen, welche kürzer sind als die Länge der Kodierer-Dekodierer-Pipeline, wird Padding genutzt, um die Eingabesequenz aufzufüllen.[3] Ein Kodierer besteht aus einem Self-Attention-Modul und einem Feedforward-Modul, während der Dekodierer aus einem Self-Attention-Modul, einem Kodierer-Dekodierer-Attention-Modul und einem Feedforward-Modul besteht.[4]"}
{"text": "Die Aufgabe des Attention-Moduls besteht darin, die Korrelation eines Eingabesymbols zu den anderen Eingabesymbolen zu berechnen, etwa die Zuordnung eines Pronomens zum zugehörigen Nomen.[4] Man unterscheidet zwischen der Einbettung (engl. embedding) { x_{i}}x_{i}, bei der es sich um das als Vektor kodierte Eingabesymbol handelt, dem Abfragevektor (engl. query) { q_{i}}q_{i}, dem Schlüsselvektor (engl. key) { k_{i}}k_i und dem Wertevektor (engl. value) . Aus jeder Einbettung werden die anderen drei Vektoren berechnet, indem diese mit während des Trainings erlernten Matrizen { Q}Q, { K}K und { V}V multipliziert werden:"}
{"text": "Das Schnabeltier (Ornithorhynchus anatinus, englisch platypus) ist ein eierlegendes Säugetier aus Australien. Es ist die einzige lebende Art der Familie der Schnabeltiere (Ornithorhynchidae). Zusammen mit den vier Arten der Ameisenigel bildet es das Taxon der Kloakentiere (Monotremata), die sich stark von allen anderen Säugetieren unterscheiden. Der Körperbau des Schnabeltiers ist flachgedrückt und stromlinienförmig, es hat gewisse Ähnlichkeiten mit einem flach gebauten Biber und hat auch einen vergleichsweise platten Schwanz. Der Körper und der Schwanz sind mit braunem, wasserabweisendem Fell bedeckt. Die Füße tragen Schwimmhäute. Die Körperlänge der Schnabeltiere beträgt rund 30 bis 40 Zentimeter, der Schwanz, der als Fettspeicher verwendet wird, ist 10 bis 15 Zentimeter lang. Schnabeltiere erreichen ein Gewicht von 0,5 bis 2,5 Kilogramm, wobei Männchen rund ein Drittel größer als Weibchen werden. Wie bei allen Kloakentieren münden bei ihnen beide Ausscheidungs- und die Geschlechtsorgane in einer gemeinsamen Öffnung, der „Kloake“. Der deutsche Name des Tieres deutet sein auffälligstes Kennzeichen bereits an, den biegsamen Schnabel, der in der Form dem einer Ente ähnelt und dessen Oberfläche etwa die Beschaffenheit von glattem Rindsleder hat. Erwachsene Schnabeltiere haben keine Zähne, sondern lediglich Hornplatten am Ober- und Unterkiefer, die zum Zermahlen der Nahrung dienen. Bei der Geburt besitzen die Tiere noch dreispitzige Backenzähne, verlieren diese jedoch im Laufe ihrer Entwicklung. Um den Schnabel effektiv nutzen zu können, ist die Kaumuskulatur der Tiere modifiziert. Die Nasenlöcher liegen auf dem Oberschnabel ziemlich weit vorn; dies ermöglicht es dem Schnabeltier, in weitgehend untergetauchtem Zustand nach dem „Schnorchel“-Prinzip zu atmen. Der Bau des Unterkiefers zeigt Ähnlichkeiten mit reptilienartigen Vorfahren. Im Gegensatz zu diesen sind die drei Gehörknöchelchen (Hammer, Amboss und Steigbügel), die bei Reptilien Teile des Kiefers bilden, allerdings fix im Schädel integriert. Dabei handelt es sich um ein Merkmal, das alle Säugetiere gemeinsam haben. Die Ohröffnung befindet sich jedoch im Vergleich zu anderen Säugern sehr nahe am Unterkiefer. Auch haben Schnabeltiere im Unterschied zu allen anderen Säugetieren zusätzliche Knochen im Schultergürtel. Die männlichen Schnabeltiere gehören zu den wenigen giftigen Säugetieren. Sie haben rund 15 Millimeter lange Giftsporne in Knöchelhöhe an den Hinterbeinen. Diese scheiden ein Gift aus, das in Drüsen im Hinterleib produziert wird. Weibliche Tiere haben bei ihrer Geburt ebenfalls Spornanlagen, verlieren diese jedoch im ersten Lebensjahr. Da das Gift nur während der Paarungszeit produziert wird, nimmt man an, dass es in erster Linie bei Kämpfen um ein paarungsbereites Weibchen eingesetzt wird. Das Gift enthält ein Peptid, das aminoterminal dem C-type natriuretic peptide (CNP, ein vasodilatatives Peptid mit bloß indirekt natriuretischer Wirkung) homolog ist.[2] Weitere fünf Proteine und Peptide wurden im Gift des Schnabeltieres identifiziert: defensin-like peptide (DLPs), Ornithorhynchus venom C-type natriuretic peptide (OvCNPs), Ornithorhynchus nerve growth factor, Hyaluronidase und l-to-d-peptide Isomerase.[3] Das Gift ist für Menschen nicht tödlich, verursacht aber sehr schmerzhafte Schwellungen, die auch mit hohen Dosen an Morphium kaum zu lindern sind und mehrere Monate bestehen können. Aus der Zeit, als die Tiere noch wegen des Schnabeltierfells gejagt wurden, gibt es Berichte, wonach Hunde, die angeschossene Tiere fangen sollten, durch das Gift starben. Wie das Gift auf andere Schnabeltiere wirkt, ist nicht bekannt; da es aber nicht zur Verteidigung gegenüber Fressfeinden, sondern bei Rivalenkämpfen eingesetzt wird, ist seine Wirkungsweise vermutlich nicht auf den Tod, sondern auf Verletzung ausgelegt."}
{"text": "The transformer building blocks are scaled dot-product attention units. When a sentence is passed into a transformer model, attention weights are calculated between every token simultaneously. The attention unit produces embeddings for every token in context that contain information about the token itself along with a weighted combination of other relevant tokens each weighted by its attention weight. For each attention unit the transformer model learns three weight matrices; the query weights, the key weights , and the value weights { W_{V}}{ W_{V}}. For each token { i}i, the input word embedding { x_{i}}x_{i} is multiplied with each of the three weight matrices to produce a query vector { q_{i}=x_{i}W_{Q}}{ q_{i}=x_{i}W_{Q}}, a key vector { k_{i}=x_{i}W_{K}}{ k_{i}=x_{i}W_{K}}, and a value vector { v_{i}=x_{i}W_{V}}{ v_{i}=x_{i}W_{V}}. Attention weights are calculated using the query and key vectors: the attention weight { a_{ij}}a_{ij} from token { i}i to token { j}j is the dot product between { q_{i}}q_{i} and { k_{j}}k_j. The attention weights are divided by the square root of the dimension of the key vectors, , which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that { W_{Q}}{ W_{Q}} and { W_{K}}{ W_{K}} are different matrices allows attention to be non-symmetric: if token { i}i attends to token { j}j (i.e.  is large), this does not necessarily mean that token { j}j will attend to token { i}i (i.e.  could be small). The output of the attention unit for token { i}i is the weighted sum of the value vectors of all tokens, weighted by { a_{ij}}a_{ij}, the attention from token { i}i to each token. The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices { Q}Q, { K}K and { V}V are defined as the matrices where the { i}ith rows are vectors { q_{i}}q_{i}, { k_{i}}k_{i}, and { v_{i}}v_{i} respectively."}
{"text": "Das ist auch ein Beispiel 6"}
{"text": "Das ist auch ein Beispiel 7"}
{"text": "Das ist auch ein Beispiel 8"}
{"text": "Das ist auch ein Beispiel 9"}
{"text": "Das ist auch ein Beispiel 10"}
{"text": "Das ist auch ein Beispiel 11"}
{"text": "Das ist auch ein Beispiel 12"}
{"text": "Das ist auch ein Beispiel 13"}
{"text": "Das ist auch ein Beispiel 14"}
{"text": "Das ist auch ein Beispiel 15"}
{"text": "Das ist auch ein Beispiel 16"}
{"text": "Das ist auch ein Beispiel 17"}
{"text": "Das ist auch ein Beispiel 18"}
{"text": "Das ist auch ein Beispiel 19"}
{"text": "Das ist auch ein Beispiel 20"}
{"text": "Das ist auch ein Beispiel 21"}
{"text": "Das ist auch ein Beispiel 22"}
{"text": "Das ist auch ein Beispiel 23"}
{"text": "Das ist auch ein Beispiel 24"}
{"text": "Das ist auch ein Beispiel 25"}
{"text": "Das ist auch ein Beispiel 26"}
{"text": "Das ist auch ein Beispiel 27"}
{"text": "Das ist auch ein Beispiel 28"}
{"text": "Das ist auch ein Beispiel 29"}
{"text": "Das ist auch ein Beispiel 30"}
{"text": "Das ist auch ein Beispiel 31"}
{"text": "Das ist auch ein Beispiel 32"}
{"text": "Das ist auch ein Beispiel 33"}
{"text": "Das ist auch ein Beispiel 34"}
{"text": "Das ist auch ein Beispiel 35"}
{"text": "Das ist auch ein Beispiel 36"}
{"text": "Das ist auch ein Beispiel 37"}
{"text": "Das ist auch ein Beispiel 38"}
{"text": "Das ist auch ein Beispiel 39"}
{"text": "Das ist auch ein Beispiel 40"}
{"text": "Das ist auch ein Beispiel 41"}
{"text": "Das ist auch ein Beispiel 42"}
{"text": "Das ist auch ein Beispiel 43"}
{"text": "Das ist auch ein Beispiel 44"}
{"text": "Das ist auch ein Beispiel 45"}
{"text": "Das ist auch ein Beispiel 46"}
{"text": "Das ist auch ein Beispiel 47"}
{"text": "Das ist auch ein Beispiel 48"}
{"text": "Das ist auch ein Beispiel 49"}
{"text": "Das ist auch ein Beispiel 50"}
{"text": "Das ist auch ein Beispiel 51"}
{"text": "Das ist auch ein Beispiel 52"}
{"text": "Das ist auch ein Beispiel 53"}
{"text": "Das ist auch ein Beispiel 54"}
{"text": "Das ist auch ein Beispiel 55"}
{"text": "Das ist auch ein Beispiel 56"}
{"text": "Das ist auch ein Beispiel 57"}
{"text": "Das ist auch ein Beispiel 58"}
{"text": "Das ist auch ein Beispiel 59"}
{"text": "Das ist auch ein Beispiel 60"}
{"text": "Das ist auch ein Beispiel 61"}
{"text": "Das ist auch ein Beispiel 62"}
{"text": "Das ist auch ein Beispiel 63"}
{"text": "Das ist auch ein Beispiel 64"}
{"text": "Das ist auch ein Beispiel 65"}
{"text": "Das ist auch ein Beispiel 66"}
{"text": "Das ist auch ein Beispiel 67"}
{"text": "Das ist auch ein Beispiel 68"}
{"text": "Das ist auch ein Beispiel 69"}
{"text": "Das ist auch ein Beispiel 70"}
{"text": "Das ist auch ein Beispiel 71"}
{"text": "Das ist auch ein Beispiel 72"}
{"text": "Das ist auch ein Beispiel 73"}
{"text": "Das ist auch ein Beispiel 74"}
{"text": "Das ist auch ein Beispiel 75"}
{"text": "Das ist auch ein Beispiel 76"}
{"text": "Das ist auch ein Beispiel 77"}
{"text": "Das ist auch ein Beispiel 78"}
{"text": "Das ist auch ein Beispiel 79"}
{"text": "Das ist auch ein Beispiel 80"}
{"text": "Das ist auch ein Beispiel 81"}
{"text": "Das ist auch ein Beispiel 82"}
{"text": "Das ist auch ein Beispiel 83"}
{"text": "Das ist auch ein Beispiel 84"}
{"text": "Das ist auch ein Beispiel 85"}
{"text": "Das ist auch ein Beispiel 86"}
{"text": "Das ist auch ein Beispiel 87"}
{"text": "Das ist auch ein Beispiel 88"}
{"text": "Das ist auch ein Beispiel 89"}
{"text": "Das ist auch ein Beispiel 90"}
{"text": "Das ist auch ein Beispiel 91"}
{"text": "Das ist auch ein Beispiel 92"}
{"text": "Das ist auch ein Beispiel 93"}
{"text": "Das ist auch ein Beispiel 94"}
{"text": "Das ist auch ein Beispiel 95"}
{"text": "Das ist auch ein Beispiel 96"}
{"text": "Das ist auch ein Beispiel 97"}
{"text": "Das ist auch ein Beispiel 98"}
{"text": "Das ist auch ein Beispiel 99"}
{"text": "Das ist auch ein Beispiel 100"}
{"text": "Das ist auch ein Beispiel 101"}
{"text": "Das ist auch ein Beispiel 102"}
{"text": "Das ist auch ein Beispiel 103"}
{"text": "Das ist auch ein Beispiel 104"}
{"text": "Das ist auch ein Beispiel 105"}
{"text": "Das ist auch ein Beispiel 106"}
{"text": "Das ist auch ein Beispiel 107"}
{"text": "Das ist auch ein Beispiel 108"}
{"text": "Das ist auch ein Beispiel 109"}
{"text": "Das ist auch ein Beispiel 110"}
{"text": "Das ist auch ein Beispiel 111"}
{"text": "Das ist auch ein Beispiel 112"}
{"text": "Das ist auch ein Beispiel 113"}
{"text": "Das ist auch ein Beispiel 114"}
{"text": "Das ist auch ein Beispiel 115"}
{"text": "Das ist auch ein Beispiel 116"}
{"text": "Das ist auch ein Beispiel 117"}
{"text": "Das ist auch ein Beispiel 118"}
{"text": "Das ist auch ein Beispiel 119"}
{"text": "Das ist auch ein Beispiel 120"}
{"text": "Das ist auch ein Beispiel 121"}
{"text": "Das ist auch ein Beispiel 122"}
{"text": "Das ist auch ein Beispiel 123"}
{"text": "Das ist auch ein Beispiel 124"}
{"text": "Das ist auch ein Beispiel 125"}
{"text": "Das ist auch ein Beispiel 126"}
{"text": "Das ist auch ein Beispiel 127"}
{"text": "Das ist auch ein Beispiel 128"}
{"text": "Das ist auch ein Beispiel 129"}
{"text": "Das ist auch ein Beispiel 130"}
{"text": "Das ist auch ein Beispiel 131"}
{"text": "Das ist auch ein Beispiel 132"}
{"text": "Das ist auch ein Beispiel 133"}
{"text": "Das ist auch ein Beispiel 134"}
{"text": "Das ist auch ein Beispiel 135"}
{"text": "Das ist auch ein Beispiel 136"}
{"text": "Das ist auch ein Beispiel 137"}
{"text": "Das ist auch ein Beispiel 138"}
{"text": "Das ist auch ein Beispiel 139"}
{"text": "Das ist auch ein Beispiel 140"}
{"text": "Das ist auch ein Beispiel 141"}
{"text": "Das ist auch ein Beispiel 142"}
{"text": "Das ist auch ein Beispiel 143"}
{"text": "Das ist auch ein Beispiel 144"}
{"text": "Das ist auch ein Beispiel 145"}
{"text": "Das ist auch ein Beispiel 146"}
{"text": "Das ist auch ein Beispiel 147"}
{"text": "Das ist auch ein Beispiel 148"}
{"text": "Das ist auch ein Beispiel 149"}
{"text": "Das ist auch ein Beispiel 150"}
{"text": "Das ist auch ein Beispiel 151"}
{"text": "Das ist auch ein Beispiel 152"}
{"text": "Das ist auch ein Beispiel 153"}
{"text": "Das ist auch ein Beispiel 154"}
{"text": "Das ist auch ein Beispiel 155"}
{"text": "Das ist auch ein Beispiel 156"}
{"text": "Das ist auch ein Beispiel 157"}
{"text": "Das ist auch ein Beispiel 158"}
{"text": "Das ist auch ein Beispiel 159"}
{"text": "Das ist auch ein Beispiel 160"}
{"text": "Das ist auch ein Beispiel 161"}
{"text": "Das ist auch ein Beispiel 162"}
{"text": "Das ist auch ein Beispiel 163"}
{"text": "Das ist auch ein Beispiel 164"}
{"text": "Das ist auch ein Beispiel 165"}
{"text": "Das ist auch ein Beispiel 166"}
{"text": "Das ist auch ein Beispiel 167"}
{"text": "Das ist auch ein Beispiel 168"}
{"text": "Das ist auch ein Beispiel 169"}
{"text": "Das ist auch ein Beispiel 170"}
{"text": "Das ist auch ein Beispiel 171"}
{"text": "Das ist auch ein Beispiel 172"}
{"text": "Das ist auch ein Beispiel 173"}
{"text": "Das ist auch ein Beispiel 174"}
{"text": "Das ist auch ein Beispiel 175"}
{"text": "Das ist auch ein Beispiel 176"}
{"text": "Das ist auch ein Beispiel 177"}
{"text": "Das ist auch ein Beispiel 178"}
{"text": "Das ist auch ein Beispiel 179"}
{"text": "Das ist auch ein Beispiel 180"}
{"text": "Das ist auch ein Beispiel 181"}
{"text": "Das ist auch ein Beispiel 182"}
{"text": "Das ist auch ein Beispiel 183"}
{"text": "Das ist auch ein Beispiel 184"}
{"text": "Das ist auch ein Beispiel 185"}
{"text": "Das ist auch ein Beispiel 186"}
{"text": "Das ist auch ein Beispiel 187"}
{"text": "Das ist auch ein Beispiel 188"}
{"text": "Das ist auch ein Beispiel 189"}
{"text": "Das ist auch ein Beispiel 190"}
{"text": "Das ist auch ein Beispiel 191"}
{"text": "Das ist auch ein Beispiel 192"}
{"text": "Das ist auch ein Beispiel 193"}
{"text": "Das ist auch ein Beispiel 194"}
{"text": "Das ist auch ein Beispiel 195"}
{"text": "Das ist auch ein Beispiel 196"}
{"text": "Das ist auch ein Beispiel 197"}
{"text": "Das ist auch ein Beispiel 198"}
{"text": "Das ist auch ein Beispiel 199"}
{"text": "Das ist auch ein Beispiel 200"}
